{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p float=\"left\">\n",
    "    <img src=\"https://earthdata.nasa.gov/img/earthdata-fb-image.jpg\" width=\"300\">\n",
    "    \n",
    "</p>\n",
    "\n",
    "## **2021 ESIP Winter Meeting: Science in the Cloud Demos**\n",
    "\n",
    "<!-- <img align=\"left\" src=\"https://earthdata.nasa.gov/img/earthdata-fb-image.jpg\" width=\"250\"> -->\n",
    "\n",
    "### Working with Cloud-Based NASA Earth Observations Data and Tools: \n",
    "#### AWS in-region access of NASA Earthdata products\n",
    "\n",
    "This notebook provides a basic end-to-end workflow to interact with data \"in-place\" from the NASA Earthdata Cloud, by accessing AWS S3 locations provided by [NASA Harmony](http://harmony.earthdata.nasa.gov/) outputs without the need to download data. While these outputs can be downloaded locally, the cloud offers the ability to scale compute resources to perform analyses over large areas and time spans, which is critical as data volumes continue to grow. \n",
    "\n",
    "This workflow combines search, discovery, access, reformatting, basic analyses, and plotting components presented during the AGU 2020 Fall Meeting Workshop \"Working with Cloud-Based NASA Earth Observations Data and Tools\", with all materials available from https://github.com/podaac/AGU-2020. Though the example we're working with in this notebook only focuses on a small time and area to account for a large number of concurrent processing requests, this workflow can be modified and scaled up to suit a larger time range and region of interest. In the example used during AGU, this simplified workflow can be scaled up in order to better understand the relationship between river discharge and sea surface salinity for impact assessment. The Learning Objectives of the original tutorial are copied below:\n",
    "\n",
    "#### Learning objectives:\n",
    "\n",
    "- Understand the Pangeo BinderHub environment used during the workshop and how to execute code within a Jupyter Notebook\n",
    "- Search for Liquid Water Equivalent (LWE) data from GRACE/GRACE-FO and Sea Surface Salinity (SSS) from SMAP \n",
    "- Execute programmatic data access queries, plotting, and direct in-region cloud access using open source Python libraries.\n",
    "- Access data in Zarr format from Earthdata Cloud (AWS)\n",
    "- Subset both, plot and compare coincident data.\n",
    "- Identify resources, including the Earthdata Cloud Primer, for getting started with Amazon Web Services outside of the Workshop to access and work with data with a cloud environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "<p float=\"left\">\n",
    "    <img src=\"https://github.com/pangeo-data/pangeo/raw/master/docs/_static/pangeo_simple_logo.svg\" width=\"200\">\n",
    "    \n",
    "</p>\n",
    "\n",
    "\n",
    "### __Pangeo BinderHub__\n",
    "\n",
    "First, some basics on the Pangeo compute environment used during the live workshop and how to interact with Jupyter Notebooks and the Jupyter Lab interface.\n",
    "\n",
    "* [Pangeo BinderHub](https://binder.pangeo.io/): A multi-user server for interactive data analysis. This Hub is running in the AWS `us-west-2` region, which is where all Earthdata Cloud data and transformation service outputs are located. Pangeo is supported, in part, by the National Science Foundation (NSF) and the National Aeronautics and Space Administration (NASA). Google provided compute credits on Google Compute Engine. The Pangeo community promotes open, reproducible, and scalable science. We thank all at Pangeo for supporting the 2020 AGU Workshop.\n",
    "\n",
    "See instructions at the bottom of this notebook for how to set up your own AWS EC2 instance so that you can perform the same cloud access within your personal AWS environment. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "### __Import modules__\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install s3fs zarr xarray intake\n",
    "\n",
    "from netrc import netrc\n",
    "from platform import system\n",
    "from getpass import getpass\n",
    "from urllib import request\n",
    "from http.cookiejar import CookieJar\n",
    "from os.path import join, expanduser\n",
    "from pprint import pprint\n",
    "import intake\n",
    "import s3fs\n",
    "#import rasterio\n",
    "import zarr\n",
    "import matplotlib.pyplot as plt\n",
    "import dask.array as da\n",
    "import time\n",
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import matplotlib.animation as animation\n",
    "# import cartopy.crs as ccrs\n",
    "# import cartopy\n",
    "import s3fs\n",
    "from IPython.display import HTML\n",
    "import json\n",
    "\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Earthdata Login__\n",
    "\n",
    "A NASA Earthdata Login account is required to access data, as well as discover restricted data, from the NASA Earthdata system. Please visit https://urs.earthdata.nasa.gov to register and manage your Earthdata Login account. This account is free to create and only takes a moment to set up.\n",
    "\n",
    "At this point in time (as we are still transitioning to a cloud environment), in order to access data from the Earthdata Cloud, you need early access permissions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Username:  amy.steiker\n",
      "Password:  ·········\n"
     ]
    }
   ],
   "source": [
    "TOKEN_DATA = (\"<token>\"\n",
    "              \"<username>%s</username>\"\n",
    "              \"<password>%s</password>\"\n",
    "              \"<client_id>PODAAC CMR Client</client_id>\"\n",
    "              \"<user_ip_address>%s</user_ip_address>\"\n",
    "              \"</token>\")\n",
    "\n",
    "\n",
    "def setup_cmr_token_auth(endpoint: str='cmr.earthdata.nasa.gov'):\n",
    "    ip = requests.get(\"https://ipinfo.io/ip\").text.strip()\n",
    "    return requests.post(\n",
    "        url=\"https://%s/legacy-services/rest/tokens\" % endpoint,\n",
    "        data=TOKEN_DATA % (input(\"Username: \"), getpass(\"Password: \"), ip),\n",
    "        headers={'Content-Type': 'application/xml', 'Accept': 'application/json'}\n",
    "    ).json()['token']['id']\n",
    "\n",
    "\n",
    "def setup_earthdata_login_auth(endpoint: str='urs.earthdata.nasa.gov'):\n",
    "    netrc_name = \"_netrc\" if system()==\"Windows\" else \".netrc\"\n",
    "    try:\n",
    "        username, _, password = netrc(file=join(expanduser('~'), netrc_name)).authenticators(endpoint)\n",
    "    except (FileNotFoundError, TypeError):\n",
    "        print('Please provide your Earthdata Login credentials for access.')\n",
    "        print('Your info will only be passed to %s and will not be exposed in Jupyter.' % (endpoint))\n",
    "        username = input('Username: ')\n",
    "        password = getpass('Password: ')\n",
    "    manager = request.HTTPPasswordMgrWithDefaultRealm()\n",
    "    manager.add_password(None, endpoint, username, password)\n",
    "    auth = request.HTTPBasicAuthHandler(manager)\n",
    "    jar = CookieJar()\n",
    "    processor = request.HTTPCookieProcessor(jar)\n",
    "    opener = request.build_opener(auth, processor)\n",
    "    request.install_opener(opener)\n",
    "\n",
    "\n",
    "# Get your authentication token for searching restricted records in the CMR:\n",
    "_token = setup_cmr_token_auth(endpoint=\"cmr.earthdata.nasa.gov\")\n",
    "\n",
    "# Start authenticated session with URS to allow restricted data downloads:\n",
    "setup_earthdata_login_auth(endpoint=\"urs.earthdata.nasa.gov\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "## __Data Search and Discovery__\n",
    "\n",
    "\n",
    "[**JPL GRACE and GRACE-FO Mascon Ocean, Ice, and Hydrology Equivalent Water Height Coastal Resolution Improvement (CRI) Filtered Release 06 Version 02**](https://podaac.jpl.nasa.gov/dataset/TELLUS_GRAC-GRFO_MASCON_CRI_GRID_RL06_V2)\n",
    "\n",
    "Provides land water equivalent (LWE) thickness for observing seasonal changes in water storage around the river. When discharge is high, the change in water storage will increase, pointing to a wet season. This product provides gridded monthly global water storage/height anomalies in a single data file in netCDF format, and can be used for analysis for ocean, ice, and hydrology phenomena. Source data are from [GRACE](https://podaac.jpl.nasa.gov/GRACE) and [GRACE-FO](https://podaac.jpl.nasa.gov/GRACE-FO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First, define the region of interest over the Amazon river basin and set the temporal range:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bounding Box spatial parameter in decimal degree 'W,S,E,N' format.\n",
    "bounding_box = '-52,-2,-43,6'\n",
    "\n",
    "# Each date in yyyy-MM-ddTHH:mm:ssZ format; date range in start,end format\n",
    "temporal = '2020-11-01T00:00:00Z,2020-11-30T23:59:59Z'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set up dictionary with the data product of interest\n",
    "\n",
    "Before we search programmatically using the [Common Metadata Repository (CMR)](https://cmr.earthdata.nasa.gov/search/site/docs/search/api.html), we can use NASA Earthdata Search to visualize file coverage over multiple data sets and to access the same data you will be working with below: \n",
    "[Earthdata Search project](https://search.earthdata.nasa.gov/search/granules?p=C1938032626-POCLOUD&pg[0][gsk]=-start_date&q=C1938032626-POCLOUD&sb[0]=-52%2C-2%2C-43%2C6&m=1.1339994258337498!-51.837100966519046!6!1!0!0%2C2&qt=2020-11-01T00%3A00%3A00.000Z%2C2020-11-30T23%3A59%3A59.999Z&tl=1595962575!4!!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_parameters = { \n",
    "        'short_name': 'TELLUS_GRAC-GRFO_MASCON_CRI_GRID_RL06_V2',\n",
    "        'provider': 'POCLOUD',\n",
    "        'bounding_box': bounding_box,\n",
    "        'temporal': temporal,\n",
    "        'token': _token,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Discover file number and file size \n",
    "\n",
    "Using CMR search, determine the number of files that exist over this time and area of interest, as well as the average size and total volume of those files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3 files\n",
      "The total size of all files is 2254.50 MB\n"
     ]
    }
   ],
   "source": [
    "search_url = \"https://cmr.earthdata.nasa.gov/search/granules\"\n",
    "output_format=\"json\"\n",
    "\n",
    "parameters = {\n",
    "        \"scroll\": \"true\",\n",
    "        \"page_size\": 100,\n",
    "}\n",
    "\n",
    "response = requests.post(f\"{search_url}.{output_format}\", params=parameters, data=search_parameters)\n",
    "response.raise_for_status()\n",
    "\n",
    "hits = int(response.headers['CMR-Hits'])\n",
    "if hits > 0:\n",
    "    print(f\"Found {hits} files\")\n",
    "    results = json.loads(response.content)\n",
    "    granules = []\n",
    "    granules.extend(results['feed']['entry'])\n",
    "    granule_sizes = [float(granule['granule_size']) for granule in granules]\n",
    "    print(f\"The total size of all files is {sum(granule_sizes):.2f} MB\")\n",
    "else:\n",
    "    print(\"Found no hits\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Locate the data access URLs provided by the data product (collection) metadata:\n",
    "\n",
    "GRACE data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "files returned: 3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'URL': 'https://archive.podaac.earthdata.nasa.gov/podaac-ops-cumulus-public/TELLUS_GRAC-GRFO_MASCON_CRI_GRID_RL06_V2/GRCTellus.JPL.200204_202009.GLO.RL06M.MSCNv02CRI.cmr.json',\n",
       "  'Type': 'EXTENDED METADATA',\n",
       "  'Description': 'File to download'},\n",
       " {'URL': 'https://archive.podaac.earthdata.nasa.gov/podaac-ops-cumulus-protected/TELLUS_GRAC-GRFO_MASCON_CRI_GRID_RL06_V2/GRCTellus.JPL.200204_202009.GLO.RL06M.MSCNv02CRI.nc',\n",
       "  'Type': 'GET DATA',\n",
       "  'Description': 'File to download'},\n",
       " {'URL': 'https://archive.podaac.earthdata.nasa.gov/podaac-ops-cumulus-public/TELLUS_GRAC-GRFO_MASCON_CRI_GRID_RL06_V2/GRCTellus.JPL.200204_202009.GLO.RL06M.MSCNv02CRI.nc.md5',\n",
       "  'Type': 'EXTENDED METADATA',\n",
       "  'Description': 'File to download'}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = requests.get(url=\"https://cmr.earthdata.nasa.gov/search/granules.umm_json\", \n",
    "                 params=search_parameters)\n",
    "grace_gran = r.json()\n",
    "print(\"files returned:\",grace_gran['hits'])\n",
    "grace_gran['items'][0]['umm']['RelatedUrls']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "## __Data Access from NASA Earthdata Harmony__\n",
    "\n",
    "There are several different methodologies to search and access data depending on your use case. The URLs above can be located with the data granule (file) metadata, and pulled into a list that can then be bulk downloaded as you've seen in the previous tutorials.\n",
    "\n",
    "[Harmony](https://harmony.earthdata.nasa.gov/) is a growing effort across the NASA EOSDIS community to provide a consistent method to access and perform data subsetting and transformation services for data in the Earthdata Cloud. These services are processed in the cloud, with data archived in the cloud, and outputs can be accessed by downloading to a local machine or through direct in-region access via Amazon Web Services. These next steps walk through this access method to access the data outputs directly within the AWS `us-west-2` region so that we can read the data into memory within this AWS environment. \n",
    "\n",
    "\n",
    "**The Harmony source code is migrating to public Github repositories to enable community contributions and development. A Python library is under development to make these next steps faster and easier.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### __Get collection ID for Harmony__\n",
    "\n",
    "Harmony operates on an input Collection concept-id (a CMR construct). Here's how you identify the CMR concept-id for the data products of interest:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GRACE collection id: C1938032626-POCLOUD\n"
     ]
    }
   ],
   "source": [
    "r = requests.get(url=\"https://cmr.earthdata.nasa.gov/search/collections.umm_json\", \n",
    "                 params=search_parameters)\n",
    "\n",
    "grace_coll = r.json()\n",
    "grace_coll['hits']\n",
    "grace_coll_meta = grace_coll['items'][0]['meta']\n",
    "grace_coll_id = grace_coll_meta['concept-id']\n",
    "print('GRACE collection id:', grace_coll_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### __Request GRACE data reformatted to Zarr__:\n",
    "\n",
    "[Zarr](https://zarr.readthedocs.io/en/stable/) is a format for the storage of chunked, compressed, N-dimensional arrays. Zarr also enables you to store arrays in memory, on disk, inside a Zip file, or on S3. Harmony's Zarr Reformatter service will transform the SMAP data from its native NetCDF format to Zarr, to allow us to open and download/read just the data that we require for our Amazon Basin study area.\n",
    "\n",
    "\n",
    "For our science use case, ideally we'd also want to request an entire year but since this can take a long time to process with many concurrent requests during this live workshop, we will just request a month of data for demonstration purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://harmony.earthdata.nasa.gov/C1938032626-POCLOUD/ogc-api-coverages/1.0.0/collections/all/coverage/rangeset?format=application/x-zarr&subset=lat(-2:6)&subset=lon(-52:-43)&subset=time(\"2019-01-01T00:00:00Z\":\"2019-12-31T23:59:59Z\")\n"
     ]
    }
   ],
   "source": [
    "harmony_root = 'https://harmony.earthdata.nasa.gov'\n",
    "harmony_params = {\n",
    "    'collection_id': grace_coll_id,\n",
    "    'ogc-api-coverages_version': '1.0.0',\n",
    "    'variable': 'all',\n",
    "    'lat':'(-2:6)',\n",
    "    'lon':'(-52:-43)',\n",
    "    'start': '2020-11-01T00:00:00Z',\n",
    "    'stop':'2020-11-30T23:59:59Z',\n",
    "    'format': 'application/x-zarr',\n",
    "}\n",
    "\n",
    "grace_url = harmony_root+'/{collection_id}/ogc-api-coverages/{ogc-api-coverages_version}/collections/{variable}/coverage/rangeset?format={format}&subset=lat{lat}&subset=lon{lon}&subset=time(\"{start}\":\"{stop}\")'.format(**harmony_params)\n",
    "print(grace_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"username\": \"amy.steiker\",\n",
      "  \"status\": \"running\",\n",
      "  \"message\": \"Data in output files may extend outside the spatial bounds you requested.\",\n",
      "  \"progress\": 0,\n",
      "  \"createdAt\": \"2021-01-27T03:47:54.295Z\",\n",
      "  \"updatedAt\": \"2021-01-27T03:47:54.295Z\",\n",
      "  \"links\": [\n",
      "    {\n",
      "      \"title\": \"Job Status\",\n",
      "      \"href\": \"https://harmony.earthdata.nasa.gov/jobs/737167e2-f8be-4fc9-9b69-5ce7f61affd1\",\n",
      "      \"rel\": \"self\",\n",
      "      \"type\": \"application/json\"\n",
      "    }\n",
      "  ],\n",
      "  \"request\": \"https://harmony.earthdata.nasa.gov/C1938032626-POCLOUD/ogc-api-coverages/1.0.0/collections/all/coverage/rangeset?format=application%2Fx-zarr&subset=lat(-2%3A6)&subset=lon(-52%3A-43)&subset=time(%222019-01-01T00%3A00%3A00Z%22%3A%222019-12-31T23%3A59%3A59Z%22)\",\n",
      "  \"jobID\": \"737167e2-f8be-4fc9-9b69-5ce7f61affd1\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "grace_response = request.urlopen(grace_url)\n",
    "grace_results = grace_response.read()\n",
    "grace_json = json.loads(grace_results)\n",
    "print(json.dumps(grace_json, indent=2))\n",
    "grace_jobId = grace_json['jobID']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Job status is running. Progress is 0 %. Trying again.\n",
      "# Job status is running. Progress is 0 %. Trying again.\n",
      "# Job status is running. Progress is 0 %. Trying again.\n",
      "# Job status is running. Progress is 0 %. Trying again.\n",
      "# Job status is running. Progress is 0 %. Trying again.\n",
      "# Job status is running. Progress is 0 %. Trying again.\n",
      "# Job status is running. Progress is 0 %. Trying again.\n",
      "# Job status is running. Progress is 0 %. Trying again.\n",
      "# Job status is running. Progress is 0 %. Trying again.\n",
      "# Job status is running. Progress is 0 %. Trying again.\n",
      "# Job status is running. Progress is 0 %. Trying again.\n",
      "# Job status is running. Progress is 0 %. Trying again.\n",
      "# Job progress is 100%. Links to job outputs are displayed below:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['https://harmony.earthdata.nasa.gov/jobs/737167e2-f8be-4fc9-9b69-5ce7f61affd1',\n",
       " 'https://harmony.earthdata.nasa.gov/stac/737167e2-f8be-4fc9-9b69-5ce7f61affd1/',\n",
       " 'https://harmony.earthdata.nasa.gov/cloud-access.sh',\n",
       " 'https://harmony.earthdata.nasa.gov/cloud-access',\n",
       " 's3://harmony-prod-staging/public/harmony/netcdf-to-zarr/aba03de6-ecc0-464e-9df2-70af686de807/',\n",
       " 's3://harmony-prod-staging/public/harmony/netcdf-to-zarr/aba03de6-ecc0-464e-9df2-70af686de807/GRCTellus.JPL.200204_202009.GLO.RL06M.MSCNv02CRI.zarr',\n",
       " 's3://harmony-prod-staging/public/harmony/netcdf-to-zarr/aba03de6-ecc0-464e-9df2-70af686de807/GRCTellus.JPL.200204_202010.GLO.RL06M.MSCNv02CRI.zarr',\n",
       " 's3://harmony-prod-staging/public/harmony/netcdf-to-zarr/aba03de6-ecc0-464e-9df2-70af686de807/GRCTellus.JPL.200204_202011.GLO.RL06M.MSCNv02CRI.zarr']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "grace_job_url = f'https://harmony.earthdata.nasa.gov/jobs/{grace_jobId}'\n",
    "\n",
    "while True:\n",
    "    loop_response = request.urlopen(grace_job_url)\n",
    "    loop_results = loop_response.read()\n",
    "    job_json = json.loads(loop_results)\n",
    "    if job_json['status'] != 'running':\n",
    "        break\n",
    "    print(f\"# Job status is running. Progress is {job_json['progress']} %. Trying again.\")\n",
    "    time.sleep(10)\n",
    "\n",
    "grace_links = []\n",
    "if job_json['status'] == 'successful' and job_json['progress'] == 100:\n",
    "    print(\"# Job progress is 100%. Links to job outputs are displayed below:\")\n",
    "    grace_links = [link['href'] for link in job_json['links']]\n",
    "    display(grace_links)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### __Access urls for the output zarr files__\n",
    "\n",
    "The zarr dataset is staged for us in an S3 bucket. The url is the last one in the list shown above.\n",
    "\n",
    "Select the url and display below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s3://harmony-prod-staging/public/harmony/netcdf-to-zarr/aba03de6-ecc0-464e-9df2-70af686de807/GRCTellus.JPL.200204_202011.GLO.RL06M.MSCNv02CRI.zarr'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grace_zarr_urls = grace_links[-1]\n",
    "grace_zarr_urls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Access credentials for the output zarr file**\n",
    "\n",
    "Credentials provided in the Harmony job response provide authenticated access to your staged S3 resources.\n",
    "\n",
    "Grab the credentials as a JSON string, load to a Python dictionary, and display their expiration date:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2021-01-27T11:54:12.000Z'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with request.urlopen(f\"https://harmony.earthdata.nasa.gov/cloud-access\") as f:\n",
    "    creds = json.loads(f.read())\n",
    "\n",
    "creds['Expiration']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Open staged zarr files with *s3fs*\n",
    "\n",
    "We use the AWS `s3fs` package to get metadata about the zarr data store and read in the credentials we pulled from our Harmony job response:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zarr_fs = s3fs.S3FileSystem(\n",
    "    key=creds['AccessKeyId'],\n",
    "    secret=creds['SecretAccessKey'],\n",
    "    token=creds['SessionToken'],\n",
    "    client_kwargs={'region_name':'us-west-2'},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GRACE data loaded into xarray\n",
    "\n",
    " `xarray` is a python package designed to work with multi-dimensional arrays.  See the [xarray website](http://xarray.pydata.org/en/stable/) for more information. This next code block will take all of the month's worth of SMAP data we pulled above from Harmony into xarray in a single command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grace_zarr_store = zarr_fs.get_mapper(root=grace_zarr_urls, check=False)\n",
    "grace_zarr_dataset = zarr.open(grace_zarr_store)\n",
    "\n",
    "print(grace_zarr_dataset.tree())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(grace_zarr_dataset.lwe_thickness.info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### __Open staged zarr file with *xarray*__\n",
    "\n",
    "Read more about `xarray`'s zarr reader here: http://xarray.pydata.org/en/stable/generated/xarray.open_zarr.html\n",
    "\n",
    "This xarray method allows you to pull in the Zarr outputs from Harmony directly into an xarray dataset. \n",
    "\n",
    "Open the zarr dataset and print the dataset \"header\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_GRACE = xr.open_zarr(grace_zarr_store)\n",
    "print(ds_GRACE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Subset by Latitude/Longitude**\n",
    "\n",
    "Once we have obtained all the data, to make processing quicker, we are going to subset datasets by latitude/longitude for the Amazon River estuary.\n",
    "\n",
    "Once we have obtained the GRACE-FO data, we should spatial subset the data to the minimal area covering the Amazon River estuary. This will reduce processing load and reduce cloud costs for the user.\n",
    "\n",
    "Make a GRACE-FO subset and display the min, max of the *lat* and *lon* variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_GRACE = ds_GRACE.sel(lat=slice(-18, 10), lon=slice(275, 330))\n",
    "print(subset_GRACE.lat.min().data, \n",
    "      subset_GRACE.lat.max().data,\n",
    "      subset_GRACE.lon.min().data,\n",
    "      subset_GRACE.lon.max().data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Select the variable for Land Water Equivalent Thickness (*lwe_thickness*)**\n",
    "\n",
    "Grab the land water equivalent thickness variable from the GRACE subset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lwe = subset_GRACE.lwe_thickness\n",
    "print(lwe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "## __Plotting and analysis__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Plot and animate the GRACE data over time__\n",
    "\n",
    "Now that we have a time/space slice of interest from the GRACE collection, let's plot the first time step, to see what we've got (or to see what it looks like). First we'll define two functions to make the plotting (and next animation step) a bit more convenient:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_map(ax, pmap, ds_subset, x, y, var, t, cmap, levels, extent):\n",
    "    title = str(pd.to_datetime(ds_subset.time[t].values))\n",
    "    pmap.set_title(title, fontsize=14)\n",
    "    pmap.coastlines()\n",
    "    pmap.set_extent(extent)\n",
    "    pmap.add_feature(cartopy.feature.RIVERS)\n",
    "    variable_desired = var[t,:,:]\n",
    "    cont = pmap.contourf(x, y, variable_desired, cmap=cmap, levels=levels, zorder=1)\n",
    "    return cont\n",
    "\n",
    "def animate_ts(framenumber, ax, pmap, ds_subset, x, y, var, t, cmap, levels, extent):\n",
    "    cont = setup_map(ax, pmap, ds_subset, x, y, var, t + framenumber, cmap, levels, extent) \n",
    "    return cont"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a matplotlib plot object and add subplot:\n",
    "fig = plt.figure(figsize=[13,9]) \n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "\n",
    "# Configure axes to display projected data using PlateCarree crs:\n",
    "pmap = plt.axes(projection=ccrs.PlateCarree())\n",
    "\n",
    "# Get arrays of x and y to label the plot axes:\n",
    "x,y = np.meshgrid(subset_GRACE.lon, subset_GRACE.lat)                        \n",
    "\n",
    "# Set a few constants for plotting the GRACE-FO time series:\n",
    "time_start  = 168\n",
    "cmap_name   = \"bwr_r\"\n",
    "cmap_levels = np.linspace(-100., 100., 14)\n",
    "map_extent  = [-85, -30, -16, 11]\n",
    "\n",
    "# Plot the first timestep: \n",
    "cont = setup_map(ax, pmap, subset_GRACE, x, y, lwe, time_start, cmap_name, cmap_levels, map_extent)\n",
    "\n",
    "fig.colorbar(cont, ticks=cmap_levels, orientation='horizontal', label='Land Water Equivalent Thickness (cm)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Animate changes over time\n",
    "\n",
    "Let's now explore how land water mass changes throughout the year 2019, by creating an animation of GRACE monthly land water equivalent (LWE) maps over the Amazon River.\n",
    "\n",
    "Plot all the 2019 timesteps sequentially to create an animation of land water equivalent thickness for the Amazon Rainforest territories:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ani = animation.FuncAnimation(fig, animate_ts, frames=range(0,12), fargs=(\n",
    "    ax, pmap, subset_GRACE, x, y, lwe, time_start, cmap_name, cmap_levels, map_extent\n",
    "), interval=500)\n",
    "\n",
    "HTML(ani.to_html5_video())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "User note: You will need to install 'ffmpeg' in the cmd prompt to save the .mpg to disk. Use the following command to install from the conda-forge channel:\n",
    "\n",
    "```shell\n",
    "conda install -c conda-forge ffmpeg\n",
    "```\n",
    "\n",
    "Uncomment, run the next cell to save the animation to MP4:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ani.save(\"earthdatacloud_animation_GRACEFO.mp4\", writer=animation.FFMpegWriter())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Tutorial Summary and Additional Resources__\n",
    "\n",
    "The building blocks are now in place to do a longer time series analysis across GRACE and SMAP data, to better understand the relationship between river discharge and sea surface salinity for impact assessment. \n",
    "\n",
    "To conclude, we've searched programmatically for data archived in the PO.DAAC Earthdata Cloud over a region and time period of interest, requested the data from the Harmony API, read the data directly into `xarray` from the staged s3 location within AWS `us-west-2` without having to pull the data down into local storage, and performed subsetting and plotting in preparation for a time series analysis. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____\n",
    "\n",
    "## __Set up for in-region access__\n",
    "\n",
    "\n",
    "__This notebook must be running within an AWS EC2 instance running in the `us-west-2` region.__\n",
    "\n",
    "The Pangeo BinderHub instance provided through the Github repository already takes care of steps 1 and 2, but these instructions are provided so that you can set this up in your own AWS account outside of the provided cloud environment.\n",
    "\n",
    "1. Follow tutorials 01 through 03 of the [NASA Earthdata Cloud Primer](https://earthdata.nasa.gov/learn/user-resources/webinars-and-tutorials/cloud-primer) to set up an EC2 instance within us-west-2. Ensure you are also following step 3 in the [\"Jupyter Notebooks on AWS EC2 in 12 (mostly easy) steps\"](https://medium.com/@alexjsanchez/python-3-notebooks-on-aws-ec2-in-15-mostly-easy-steps-2ec5e662c6c6) article to set the correct security group settings needed to connect your local port to your EC2’s notebook port thru SSH.\n",
    "\n",
    "2. Follow the remaining instructions in the Medium article above up until Step 11 (running Jupyter Lab). These instructions include installation of Anaconda3 (including Jupyter Lab) in your ec2 instance. Note the following updates and suggestions:\n",
    "    * Step 5: Type the following command instead of what is suggested in the article: `ssh -i \"tutorialexample.pem\" ec2-user@ec2-54-144-47-199.compute-1.amazonaws.com -L 9999:localhost:8888`. This will eliminate the need to create a ssh config file in your home directory (Step 10).\n",
    "    * As of December 2020, the most current Anaconda3 Linux distribution is: https://repo.anaconda.com/archive/Anaconda3-2020.11-Linux-x86_64.sh\n",
    "    * The Anaconda installation prompts are not the same as in the article. You will not be prompted to include Anaconda3 in your .bashrc PATH so you can skip to their step 9. Instead select \"yes\" to initialize Anaconda by running `conda init`. \n",
    "    * For this notebook in particular, the following package installations are needed:\n",
    "    \n",
    "    `conda install -c conda-forge cartopy`\n",
    "    \n",
    "    `conda install -c conda-forge ffmpeg`\n",
    "    \n",
    "    `pip install s3fs zarr xarray pystac intake`\n",
    "    \n",
    "    * In order to clone the notebook within Jupyter Lab, git is needed:\n",
    "    \n",
    "    `sudo yum install git`\n",
    "Before moving over to Jupyter Lab, set up your Earthdata Login authentication and Harmony access keys:\n",
    "\n",
    "3. Setup your `~/.netrc` for Earthdata Login in your ec2 instance:\n",
    "\n",
    "```\n",
    "cd ~\n",
    "touch .netrc\n",
    "echo \"machine urs.earthdata.nasa.gov login uid_goes_here password password_goes_here\" > .netrc\n",
    "chmod 0600 .netrc\n",
    "```\n",
    "\n",
    "4. Run the following in your ec2 instance terminal window to generate short-term Harmony access keys:\n",
    "\n",
    "`curl -Ln -bj https://harmony.earthdata.nasa.gov/cloud-access.sh`\n",
    "\n",
    "5. Set your environment variables based on the keys provided in step 4:\n",
    "\n",
    "`export AWS_ACCESS_KEY_ID='...\n",
    "export AWS_SECRET_ACCESS_KEY='...'\n",
    "export AWS_SESSION_TOKEN='...'\n",
    "export AWS_DEFAULT_REGION='us-west-2'`\n",
    "\n",
    "Note that these expire within 8 hours of the script generation.\n",
    "\n",
    "6. Launch jupyter lab:\n",
    "\n",
    "`jupyter lab --no-browser`\n",
    "\n",
    "Copy the URL that begins with `http://localhost:8888` into a browser window. Replace `8888` with `9999`. \n",
    "\n",
    "You should now be up and running with JupyterLab in your EC2!\n",
    "\n",
    "7. Clone this repository\n",
    "\n",
    "Open a Terminal in the Jupyter Lab launcher and enter the following git clone command:\n",
    "\n",
    "`git clone https://github.com/podaac/AGU-2020.git`\n",
    "\n",
    "The repository, including all notebooks, are now cloned and can be opened within Jupyter Lab.\n",
    "****"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
