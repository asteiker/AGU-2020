{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img align=\"left\" src=\"https://earthdata.nasa.gov/img/earthdata-fb-image.jpg\" width=\"250\">\n",
    "\n",
    "# __Amazon river freshwater discharge impacts on coastal waters:__\n",
    "\n",
    "## __Hands-on tutorial of AWS in-region access of NASA Earthdata products__\n",
    "\n",
    "\n",
    "This notebook provides a basic end-to-end workflow to interact with data \"in-place\" from the NASA Earthdata Cloud, by accessing AWS S3 locations provided by [NASA Harmony](http://harmony.earthdata.nasa.gov/) outputs without the need to download data. While these outputs can be downloaded locally, the cloud offers the ability to scale compute resources to perform analyses over large areas and time spans, which is critical as data volumes continue to grow. \n",
    "\n",
    "This workflow combines search, discovery, access, reformatting, basic analyses, and plotting components presented during Part-II. Though the example we're working with in this notebook only focuses on a small time and area to account for a large number of concurrent processing requests, this workflow can be modified and scaled up to suit a larger time range and region of interest. \n",
    "\n",
    "#### Learning objectives:\n",
    "\n",
    "- Understand the Pangeo BinderHub environment used during the workshop and how to execute code within a Jupyter Notebook\n",
    "- Search for Liquid Water Equivalent (LWE) data from GRACE/GRACE-FO and Sea Surface Salinity (SSS) from SMAP \n",
    "- Execute programmatic data access queries, plotting, and direct in-region cloud access using open source Python libraries.\n",
    "- Access data in Zarr format from Earthdata Cloud (AWS)\n",
    "- Subset both, plot and compare coincident data.\n",
    "- Identify resources, including the Earthdata Cloud Primer, for getting started with Amazon Web Services outside of the Workshop to access and work with data with a cloud environment.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "<p float=\"left\">\n",
    "    <img src=\"https://jupyter.org/assets/main-logo.svg\" width=\"100\">\n",
    "    <img src=\"https://github.com/pangeo-data/pangeo/raw/master/docs/_static/pangeo_simple_logo.svg\" width=\"200\">\n",
    "    \n",
    "</p>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### __Pangeo BinderHub and Project Jupyter__\n",
    "\n",
    "First, some basics on the Pangeo compute environment used during the live workshop and how to interact with Jupyter Notebooks and the Jupyter Lab interface.\n",
    "\n",
    "* [Pangeo BinderHub](https://binder.pangeo.io/): A multi-user server for interactive data analysis. This Hub is running in the AWS `us-west-2` region, which is where all Earthdata Cloud data and transformation service outputs are located. Pangeo is supported, in part, by the National Science Foundation (NSF) and the National Aeronautics and Space Administration (NASA). Google provided compute credits on Google Compute Engine. The Pangeo community promotes open, reproducible, and scalable science. We thank you for supporting this AGU Workshop.\n",
    "\n",
    "**This Hub is only supported during the live AGU workshop**. See instructions at the bottom of this notebook for how to set up your own AWS EC2 instance so that you can perform the same cloud access within your personal AWS environment. \n",
    "\n",
    "* [Jupyter Notebook](https://jupyter-notebook.readthedocs.io/en/latest/): Interactive, reproducible, open source, and exploratory browser integrated computing environment.\n",
    "* [JupyterLab](https://github.com/jupyterlab/jupyterlab): Web-based integrated IDE for computational workflows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __Jupyter Notebook Basics__\n",
    "\n",
    "The body of a notebook is composed of cells. Each cell contains either markdown, code input, code output, or raw text. Cells can be included in any order and edited and executed at-will.\n",
    "\n",
    "**Markdown cells** - These are used to build a nicely formatted narrative around the code in the document.\n",
    "\n",
    "**Code cells** - These are used to define the computational code in the document. They come in two forms: the input cell where the user types the code to be executed, and the output cell which is the representation of the executed code.\n",
    "\n",
    "**Raw cells** - These are used when text needs to be included in raw form, without execution or transformation.\n",
    "\n",
    "#### Execute a cell or selected cells by pressing shift + enter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Hello World!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Collapse a cell or cell output by clicking on the blue line to the left of the cell\n",
    "\n",
    "The cell content is replaced by three dots, indicating that the cell is collapsed.\n",
    "\n",
    "#### Execute multiple cells or run the entire notebook\n",
    "Select cells with **shift + Up** or **shift + Down** and then execute selection with **shift + enter**.\n",
    "\n",
    "#### Run the whole notebook in a single step by clicking on the menu Run -> Run All Cells.\n",
    "\n",
    "See https://jupyter.readthedocs.io/en/latest/running.html for more guidance on running notebooks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "### __Import modules__\n",
    "\n",
    "The Python ecosystem is organized into modules.  A module must be imported before the contents of that modules can be used.  It is good practice to import modules in the first code cell of a notebook or at the top of your script.  Not only does this make it clear which modules are being used, but it also ensures that the code fails at the beginning because one of the modules is not installed rather half way through after crunching a load of data.\n",
    "\n",
    "For some modules, it is common practice to shorten the module names according to accepted conventions.  For example, the plotting module `matplotlib.pyplot` is shortened to `plt`.  It is best to stick to these conventions rather than making up your own short names so that people reading your code see immediately what you are doing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from netrc import netrc\n",
    "from platform import system\n",
    "from getpass import getpass\n",
    "from urllib import request\n",
    "from http.cookiejar import CookieJar\n",
    "from os.path import join, expanduser\n",
    "from pprint import pprint\n",
    "import intake\n",
    "import s3fs\n",
    "import rasterio\n",
    "import zarr\n",
    "import matplotlib.pyplot as plt\n",
    "import dask.array as da\n",
    "import time\n",
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import matplotlib.animation as animation\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy\n",
    "import zarr\n",
    "import s3fs\n",
    "from IPython.display import HTML\n",
    "import json\n",
    "\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Earthdata Login__\n",
    "\n",
    "An Earthdata Login account is required to access data, as well as discover restricted data, from the NASA Earthdata system. Please visit https://urs.earthdata.nasa.gov to register and manage your Earthdata Login account. This account is free to create and only takes a moment to set up.\n",
    "\n",
    "At this point in time (as we are still transitioning to a cloud environment), in order to access data from the Earthdata Cloud, you need special, early access, persmissions. For the workshop today, you have already been added to the list and the Earthdata login you provided prior to the workshop has been granted this access.\n",
    "\n",
    "The `setup_earthdata_login_auth` function will allow Python scripts to log into any Earthdata Login application programmatically.  To avoid being prompted for\n",
    "credentials every time you run and also allow clients such as curl to log in, you can add the following\n",
    "to a `.netrc` (`_netrc` on Windows) file in your home directory:\n",
    "\n",
    "```\n",
    "machine urs.earthdata.nasa.gov\n",
    "    login <your username>\n",
    "    password <your password>\n",
    "```\n",
    "\n",
    "Make sure that this file is only readable by the current user or you will receive an error stating\n",
    "\"netrc access too permissive.\"\n",
    "\n",
    "`$ chmod 0600 ~/.netrc` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOKEN_DATA = (\"<token>\"\n",
    "              \"<username>%s</username>\"\n",
    "              \"<password>%s</password>\"\n",
    "              \"<client_id>PODAAC CMR Client</client_id>\"\n",
    "              \"<user_ip_address>%s</user_ip_address>\"\n",
    "              \"</token>\")\n",
    "\n",
    "\n",
    "def setup_cmr_token_auth(endpoint: str='cmr.earthdata.nasa.gov'):\n",
    "    ip = requests.get(\"https://ipinfo.io/ip\").text.strip()\n",
    "    return requests.post(\n",
    "        url=\"https://%s/legacy-services/rest/tokens\" % endpoint,\n",
    "        data=TOKEN_DATA % (input(\"Username: \"), getpass(\"Password: \"), ip),\n",
    "        headers={'Content-Type': 'application/xml', 'Accept': 'application/json'}\n",
    "    ).json()['token']['id']\n",
    "\n",
    "\n",
    "def setup_earthdata_login_auth(endpoint: str='urs.earthdata.nasa.gov'):\n",
    "    netrc_name = \"_netrc\" if system()==\"Windows\" else \".netrc\"\n",
    "    try:\n",
    "        username, _, password = netrc(file=join(expanduser('~'), netrc_name)).authenticators(endpoint)\n",
    "    except (FileNotFoundError, TypeError):\n",
    "        print('Please provide your Earthdata Login credentials for access.')\n",
    "        print('Your info will only be passed to %s and will not be exposed in Jupyter.' % (endpoint))\n",
    "        username = input('Username: ')\n",
    "        password = getpass('Password: ')\n",
    "    manager = request.HTTPPasswordMgrWithDefaultRealm()\n",
    "    manager.add_password(None, endpoint, username, password)\n",
    "    auth = request.HTTPBasicAuthHandler(manager)\n",
    "    jar = CookieJar()\n",
    "    processor = request.HTTPCookieProcessor(jar)\n",
    "    opener = request.build_opener(auth, processor)\n",
    "    request.install_opener(opener)\n",
    "\n",
    "\n",
    "# Get your authentication token for searching restricted records in the CMR:\n",
    "_token = setup_cmr_token_auth(endpoint=\"cmr.earthdata.nasa.gov\")\n",
    "\n",
    "# Start authenticated session with URS to allow restricted data downloads:\n",
    "setup_earthdata_login_auth(endpoint=\"urs.earthdata.nasa.gov\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "## __Data Search and Discovery__\n",
    "\n",
    "Background on the two datasets we're working with in this tutorial:\n",
    "\n",
    "[**JPL GRACE and GRACE-FO Mascon Ocean, Ice, and Hydrology Equivalent Water Height Coastal Resolution Improvement (CRI) Filtered Release 06 Version 02**](https://podaac.jpl.nasa.gov/dataset/TELLUS_GRAC-GRFO_MASCON_CRI_GRID_RL06_V2)\n",
    "\n",
    "Provides land water equivalent (LWE) thickness for observing seasonal changes in water storage around the river. When discharge is high, the change in water storage will increase, pointing to a wet season. This product provides gridded monthly global water storage/height anomalies in a single data file in netCDF format, and can be used for analysis for ocean, ice, and hydrology phenomena. Source data are from [GRACE](https://podaac.jpl.nasa.gov/GRACE) and [GRACE-FO](https://podaac.jpl.nasa.gov/GRACE-FO)\n",
    "\n",
    "[**RSS SMAP Level 3 Sea Surface Salinity Standard Mapped Image 8-Day Running Mean V4.0 Validated Dataset**](https://podaac.jpl.nasa.gov/SMAP?sections=data)\n",
    "\n",
    "Daily data files (NetCDF format) for this product are based on Sea Surface Salinity averages spanning an 8-day moving time window, gridded at 0.25 degree x 0.25 degree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First, define the region of interest over the Amazon river basin and set the temporal range for the year 2019:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bounding Box spatial parameter in decimal degree 'W,S,E,N' format.\n",
    "bounding_box = '-52,-2,-43,6'\n",
    "\n",
    "# Each date in yyyy-MM-ddTHH:mm:ssZ format; date range in start,end format\n",
    "temporal = '2019-01-01T00:00:00Z,2019-12-31T23:59:59Z'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set up a nested dictionary with the two data products of interest\n",
    "\n",
    "Before we search programmatically using the [Common Metadata Repository (CMR)](https://cmr.earthdata.nasa.gov/search/site/docs/search/api.html), we can use NASA Earthdata Search to visualize file coverage over multiple data sets and to access the same data you will be working with below: \n",
    "[Earthdata Search project](https://search.earthdata.nasa.gov/projects?p=!C1938032626-POCLOUD!C1940468263-POCLOUD!C1650311642-PODAAC!C1664148252-PODAAC&pg[1][v]=t&pg[1][gsk]=-start_date&pg[2][v]=t&pg[2][gsk]=-start_date&pg[3][v]=t&pg[4][v]=t&q=TELLUS_GRAC-GRFO_MASCON_CRI_GRID_RL06_V2&sb[0]=-52%2C-2%2C-43%2C6&m=2.00390625!-47.50048828125!6!1!0!0%2C2&qt=2019-04-01T00%3A00%3A00.000Z%2C2019-04-30T23%3A59%3A59.999Z&tl=1591244551!4!!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_parameters = { \n",
    "    'grace': {\n",
    "        'short_name': 'TELLUS_GRAC-GRFO_MASCON_CRI_GRID_RL06_V2',\n",
    "        'provider': 'POCLOUD',\n",
    "        'bounding_box': bounding_box,\n",
    "        'temporal': temporal,\n",
    "        'token': _token},\n",
    "    'smap': {\n",
    "        'short_name': 'SMAP_RSS_L3_SSS_SMI_8DAY-RUNNINGMEAN_V4',\n",
    "        'provider': 'POCLOUD',\n",
    "        'bounding_box': bounding_box,\n",
    "        'temporal': temporal,\n",
    "        'token': _token},\n",
    "            }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Discover file number and file size \n",
    "\n",
    "Using CMR search, determine the number of files that exist over this time and area of interest, as well as the average size and total volume of those files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_url = \"https://cmr.earthdata.nasa.gov/search/granules\"\n",
    "output_format=\"json\"\n",
    "\n",
    "for k, v in search_parameters.items(): #fn.search_granules(search_parameters[k], _token)\n",
    "    parameters = {\n",
    "        \"scroll\": \"true\",\n",
    "        \"page_size\": 100,\n",
    "    }\n",
    "\n",
    "    response = requests.post(f\"{search_url}.{output_format}\", params=parameters, data=v)\n",
    "    response.raise_for_status()\n",
    "\n",
    "    hits = int(response.headers['CMR-Hits'])\n",
    "    if hits > 0:\n",
    "        print(f\"Found {hits} files\")\n",
    "        results = json.loads(response.content)\n",
    "        granules = []\n",
    "        granules.extend(results['feed']['entry'])\n",
    "        granule_sizes = [float(granule['granule_size']) for granule in granules]\n",
    "        print(f\"The total size of all files is {sum(granule_sizes):.2f} MB\")\n",
    "    else:\n",
    "        print(\"Found no hits\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Locate the data access URLs provided by the data product (collection) metadata:\n",
    "\n",
    "GRACE data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = requests.get(url=\"https://cmr.earthdata.nasa.gov/search/granules.umm_json\", \n",
    "                 params=search_parameters['grace'])\n",
    "grace_gran = r.json()\n",
    "print(\"files returned:\",grace_gran['hits'])\n",
    "grace_gran['items'][0]['umm']['RelatedUrls']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and SMAP data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = requests.get(url=\"https://cmr.earthdata.nasa.gov/search/granules.umm_json\", \n",
    "                 params=search_parameters['smap'])\n",
    "\n",
    "smap_gran = r.json()\n",
    "print(\"files returned:\",smap_gran['hits'])\n",
    "smap_gran['items'][0]['umm']['RelatedUrls']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "## __Data Access from NASA Earthdata Harmony__\n",
    "\n",
    "As you have seen so far in this workshop, there are several different methodologies to search and access data. The URLs above can be located with the data granule (file) metadata, and pulled into a list that can then be bulk downloaded as you've seen in the previous tutorials.\n",
    "\n",
    "[Harmony](https://harmony.earthdata.nasa.gov/) is a growing effort across NASA EOSDIS community to provide a consistent method to access and perform data subsetting and transformation services for data in the Earthdata Cloud. These services are processed in the cloud, with data archived in the cloud, and outputs can be accessed by downloading to a local machine or through direct in-region access via Amazon Web Services. These next steps walk through this access method to access the data outputs directly within the AWS `us-west-2` region so that we can read the data into memory within this AWS hub environment. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### __Get collection ID for Harmony__\n",
    "\n",
    "Harmony operates on an input Collection concept-id (a CMR construct). Here's how you identify the CMR concept-id for the data products of interest:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = requests.get(url=\"https://cmr.earthdata.nasa.gov/search/collections.umm_json\", \n",
    "                 params=search_parameters['grace'])\n",
    "\n",
    "grace_coll = r.json()\n",
    "grace_coll['hits']\n",
    "grace_coll_meta = grace_coll['items'][0]['meta']\n",
    "grace_coll_id = grace_coll_meta['concept-id']\n",
    "print('GRACE collection id:', grace_coll_id)\n",
    "\n",
    "r = requests.get(url=\"https://cmr.earthdata.nasa.gov/search/collections.umm_json\", \n",
    "                 params=search_parameters['smap'])\n",
    "\n",
    "smap_coll = r.json()\n",
    "smap_coll['hits']\n",
    "\n",
    "smap_coll_meta = smap_coll['items'][0]['meta']\n",
    "smap_coll_id = smap_coll_meta['concept-id']\n",
    "print('SMAP collection id:', smap_coll_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### __Request SMAP data reformatted to Zarr__:\n",
    "\n",
    "[Zarr](https://zarr.readthedocs.io/en/stable/) is a format for the storage of chunked, compressed, N-dimensional arrays. Zarr also enables you to store arrays in memory, on disk, inside a Zip file, or on S3. Harmony's Zarr Reformatter service will transform the SMAP data from its native NetCDF format to Zarr, to allow us to open and download/read just the data that we require for our Amazon Basin study area.\n",
    "\n",
    "\n",
    "For our science use case, ideally we'd also want to request an entire year but since this can take a long time to process with many concurrent requests during this live workshop, we will just request a month of data for demonstration purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "harmony_root = 'https://harmony.earthdata.nasa.gov'\n",
    "harmony_params = {\n",
    "    'collection_id': smap_coll_id,\n",
    "    'ogc-api-coverages_version': '1.0.0',\n",
    "    'variable': 'all',\n",
    "    'lat':'(-2:6)',\n",
    "    'lon':'(-52:-43)',\n",
    "    'start': '2019-04-01T00:00:00Z',\n",
    "    'stop':'2019-04-30T23:59:59Z',\n",
    "    'format': 'application/x-zarr',\n",
    "}\n",
    "\n",
    "smap_url = harmony_root+'/{collection_id}/ogc-api-coverages/{ogc-api-coverages_version}/collections/{variable}/coverage/rangeset?format={format}&subset=lat{lat}&subset=lon{lon}&subset=time(\"{start}\":\"{stop}\")'.format(**harmony_params)\n",
    "print(smap_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smap_response = request.urlopen(smap_url)\n",
    "smap_results = smap_response.read()\n",
    "smap_json = json.loads(smap_results)\n",
    "print(json.dumps(smap_json, indent=2))\n",
    "smap_jobId = smap_json['jobID']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smap_job_url = f'https://harmony.earthdata.nasa.gov/jobs/{smap_jobId}'\n",
    "\n",
    "while True:\n",
    "    loop_response = request.urlopen(smap_job_url)\n",
    "    loop_results = loop_response.read()\n",
    "    job_json = json.loads(loop_results)\n",
    "    if job_json['status'] != 'running':\n",
    "        break\n",
    "    print(f\"# Job status is running. Progress is {job_json['progress']} %. Trying again.\")\n",
    "    time.sleep(10)\n",
    "\n",
    "smap_links = []\n",
    "if job_json['status'] == 'successful' and job_json['progress'] == 100:\n",
    "    print(\"# Job progress is 100%. Links to job outputs are displayed below:\")\n",
    "    smap_links = [link['href'] for link in job_json['links']]\n",
    "    display(smap_links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smap_zarr_urls = smap_links[5:]\n",
    "smap_zarr_urls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Access credentials for the output zarr file**\n",
    "\n",
    "Credentials provided in the Harmony job response provide authenticated access to your staged S3 resources.\n",
    "\n",
    "Grab the credentials as a JSON string, load to a Python dictionary, and display their expiration date:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with request.urlopen(f\"https://harmony.earthdata.nasa.gov/cloud-access\") as f:\n",
    "    creds = json.loads(f.read())\n",
    "\n",
    "creds['Expiration']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Open staged zarr files with *s3fs*\n",
    "\n",
    "We use the AWS `s3fs` package to get metadata about the zarr data store and read in the credentials we pulled from our Harmony job response:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zarr_fs = s3fs.S3FileSystem(\n",
    "    key=creds['AccessKeyId'],\n",
    "    secret=creds['SecretAccessKey'],\n",
    "    token=creds['SessionToken'],\n",
    "    client_kwargs={'region_name':'us-west-2'},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SMAP loaded into xarray\n",
    "\n",
    " `xarray` is a python package designed to work with multi-dimensional arrays.  See the [xarray website](http://xarray.pydata.org/en/stable/) for more information. This next code block will take all of the month's worth of SMAP data we pulled above from Harmony into xarray in a single command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smap_zarr_urls = smap_links[5:]\n",
    "smap_zarr_stores = [zarr_fs.get_mapper(root=u, check=False) for u in smap_zarr_urls]\n",
    "ds_SMAP = xr.open_mfdataset(smap_zarr_stores, engine=\"zarr\")\n",
    "\n",
    "print(ds_SMAP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____\n",
    "### __We'll come back to our SMAP data now that it's loaded into xarray.__\n",
    "\n",
    "**We'll now move into breakout rooms to work through the same data acces steps as above with the GRACE data.**\n",
    "\n",
    "As you run through each of the code blocks, worth within your groups to ask questions and discuss the workflow. Instructors will move in and out of breakout rooms to offer assistance. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### __Request GRACE data reformatted to Zarr__:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "harmony_root = 'https://harmony.earthdata.nasa.gov'\n",
    "harmony_params = {\n",
    "    'collection_id': grace_coll_id,\n",
    "    'ogc-api-coverages_version': '1.0.0',\n",
    "    'variable': 'all',\n",
    "    'lat':'(-2:6)',\n",
    "    'lon':'(-52:-43)',\n",
    "    'start': '2019-01-01T00:00:00Z',\n",
    "    'stop':'2019-12-31T23:59:59Z',\n",
    "    'format': 'application/x-zarr',\n",
    "}\n",
    "\n",
    "grace_url = harmony_root+'/{collection_id}/ogc-api-coverages/{ogc-api-coverages_version}/collections/{variable}/coverage/rangeset?format={format}&subset=lat{lat}&subset=lon{lon}&subset=time(\"{start}\":\"{stop}\")'.format(**harmony_params)\n",
    "print(grace_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grace_response = request.urlopen(grace_url)\n",
    "grace_results = grace_response.read()\n",
    "grace_json = json.loads(grace_results)\n",
    "print(json.dumps(grace_json, indent=2))\n",
    "grace_jobId = grace_json['jobID']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grace_job_url = f'https://harmony.earthdata.nasa.gov/jobs/{grace_jobId}'\n",
    "\n",
    "while True:\n",
    "    loop_response = request.urlopen(grace_job_url)\n",
    "    loop_results = loop_response.read()\n",
    "    job_json = json.loads(loop_results)\n",
    "    if job_json['status'] != 'running':\n",
    "        break\n",
    "    print(f\"# Job status is running. Progress is {job_json['progress']} %. Trying again.\")\n",
    "    time.sleep(10)\n",
    "\n",
    "grace_links = []\n",
    "if job_json['status'] == 'successful' and job_json['progress'] == 100:\n",
    "    print(\"# Job progress is 100%. Links to job outputs are displayed below:\")\n",
    "    grace_links = [link['href'] for link in job_json['links']]\n",
    "    display(grace_links)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### __Access urls for the output zarr files__\n",
    "\n",
    "The new zarr dataset is staged for us in an S3 bucket. The url is the last one in the list shown above.\n",
    "\n",
    "Select the url and display below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grace_zarr_urls = grace_links[-1]\n",
    "grace_zarr_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grace_zarr_store = zarr_fs.get_mapper(root=grace_zarr_urls, check=False)\n",
    "grace_zarr_dataset = zarr.open(grace_zarr_store)\n",
    "\n",
    "print(grace_zarr_dataset.tree())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(grace_zarr_dataset.lwe_thickness.info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### __Open staged zarr file with *xarray*__\n",
    "\n",
    "Read more about `xarray`'s zarr reader here: http://xarray.pydata.org/en/stable/generated/xarray.open_zarr.html\n",
    "\n",
    "This xarray method allows you to pull in the Zarr outputs from Harmony directly into an xarray dataset. \n",
    "\n",
    "Open the zarr dataset and print the dataset \"header\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_GRACE = xr.open_zarr(grace_zarr_store)\n",
    "print(ds_GRACE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Subset by Latitude/Longitude**\n",
    "\n",
    "Once we have obtained all the data, to make processing quicker, we are going to subset datasets by latitude/longitude for the Amazon River estuary.\n",
    "\n",
    "Once we have obtained the GRACE-FO data, we should spatial subset the data to the minimal area covering the Amazon River estuary. This will reduce processing load and reduce cloud costs for the user.\n",
    "\n",
    "Make a GRACE-FO subset and display the min, max of the *lat* and *lon* variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_GRACE = ds_GRACE.sel(lat=slice(-18, 10), lon=slice(275, 330))\n",
    "print(subset_GRACE.lat.min().data, \n",
    "      subset_GRACE.lat.max().data,\n",
    "      subset_GRACE.lon.min().data,\n",
    "      subset_GRACE.lon.max().data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Select the variable for Land Water Equivalent Thickness (*lwe_thickness*)**\n",
    "\n",
    "Grab the land water equivalent thickness variable from the GRACE subset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lwe = subset_GRACE.lwe_thickness\n",
    "print(lwe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __Return to the main room__\n",
    "\n",
    "Now that the GRACE data have also been loaded into xarray and subsetted, we'll return to the main room to begin our plotting and analysis.\n",
    "____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Plotting and analysis__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__First we'll plot and animate the GRACE data over time, and then we'll subset and plot the SMAP data.__\n",
    "\n",
    "Now that we have a time/space slice of interest from the GRACE collection, let's plot the first time step, to see what we've got (or to see what it looks like. First we'll define two functions to make the plotting (and next animation step) a bit more convenient:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_map(ax, pmap, ds_subset, x, y, var, t, cmap, levels, extent):\n",
    "    title = str(pd.to_datetime(ds_subset.time[t].values))\n",
    "    pmap.set_title(title, fontsize=14)\n",
    "    pmap.coastlines()\n",
    "    pmap.set_extent(extent)\n",
    "    pmap.add_feature(cartopy.feature.RIVERS)\n",
    "    variable_desired = var[t,:,:]\n",
    "    cont = pmap.contourf(x, y, variable_desired, cmap=cmap, levels=levels, zorder=1)\n",
    "    return cont\n",
    "\n",
    "def animate_ts(framenumber, ax, pmap, ds_subset, x, y, var, t, cmap, levels, extent):\n",
    "    cont = setup_map(ax, pmap, ds_subset, x, y, var, t + framenumber, cmap, levels, extent) \n",
    "    return cont"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a matplotlib plot object and add subplot:\n",
    "fig = plt.figure(figsize=[13,9]) \n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "\n",
    "# Configure axes to display projected data using PlateCarree crs:\n",
    "pmap = plt.axes(projection=ccrs.PlateCarree())\n",
    "\n",
    "# Get arrays of x and y to label the plot axes:\n",
    "x,y = np.meshgrid(subset_GRACE.lon, subset_GRACE.lat)                        \n",
    "\n",
    "# Set a few constants for plotting the GRACE-FO time series:\n",
    "time_start  = 168\n",
    "cmap_name   = \"bwr_r\"\n",
    "cmap_levels = np.linspace(-100., 100., 14)\n",
    "map_extent  = [-85, -30, -16, 11]\n",
    "\n",
    "# Plot the first timestep: \n",
    "cont = setup_map(ax, pmap, subset_GRACE, x, y, lwe, time_start, cmap_name, cmap_levels, map_extent)\n",
    "\n",
    "fig.colorbar(cont, ticks=cmap_levels, orientation='horizontal', label='Land Water Equivalent Thickness (cm)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Animate changes over time\n",
    "\n",
    "Let's now explore how land water mass changes throughout the year 2019, by creating an animation of GRACE monthly land water equivalent (LWE) maps over the Amazon River.\n",
    "\n",
    "Plot all the 2019 timesteps sequentially to create an animation of land water equivalent thickness for the Amazon Rainforest territories:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ani = animation.FuncAnimation(fig, animate_ts, frames=range(0,12), fargs=(\n",
    "    ax, pmap, subset_GRACE, x, y, lwe, time_start, cmap_name, cmap_levels, map_extent\n",
    "), interval=500)\n",
    "\n",
    "HTML(ani.to_html5_video())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "User note: You will need to install 'ffmpeg' in the cmd prompt to save the .mpg to disk. Use the following command to install from the conda-forge channel:\n",
    "\n",
    "```shell\n",
    "conda install -c conda-forge ffmpeg\n",
    "```\n",
    "\n",
    "Uncomment, run the next cell to save the animation to MP4:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ani.save(\"earthdatacloud_animation_GRACEFO.mp4\", writer=animation.FFMpegWriter())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __SMAP Data Analysis__\n",
    "\n",
    "Normally, we'd pull the entire 2019 data for the SMAP collections as well (recall we only requested 1 month of data earlier in the notebook, due to current system constraints), average the daily SMAP data to monthly means, and compare with the monthly GRACE in a 2019 monthly time series.\n",
    "\n",
    "For now, let's create the building blocks for that workflow, by creating the 1 monthly mean for SMAP, plotting a spatial map of the SMAP SSS monthly mean to take a quick look at that data, and setting up the 2019 monthly time series plotting (without completing here)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Frist, we can subset the SMAP data as we did above with GRACE. Note the coordinate syntax differs slightly as noted below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SMAP\n",
    "lat_bnds, lon_bnds = [-2, 6], [180-52, 180-43] #switched lat directions from GRACE, and longitude has positives and negatives\n",
    "ds_SMAP_subset = ds_SMAP.sel(lat=slice(*lat_bnds), lon=slice(*lon_bnds))\n",
    "print(ds_SMAP_subset.lat.min().data,\n",
    "      ds_SMAP_subset.lat.max().data,\n",
    "      ds_SMAP_subset.lon.min().data,\n",
    "      ds_SMAP_subset.lon.max().data,)\n",
    "ds_SMAP_subset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Important**: remember to use `compute()` so that the `scale_factor` is applied!\n",
    "\n",
    "We'll also replace fill values with numpy.nan for our next monthly mean computation step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the values to their valid range (compute):\n",
    "sss_data = ds_SMAP_subset.sss_smap.data.compute()\n",
    "\n",
    "# Replace fills with numpy nan:\n",
    "sss_data[sss_data==-9999.] = np.nan\n",
    "\n",
    "# Replace the data array for the sss monthly mean variable:\n",
    "ds_SMAP_subset.sss_smap.data = sss_data\n",
    "\n",
    "print(ds_SMAP_subset.sss_smap)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now compute monthly means using the convenient xarray `groupby` method and view the minimum and maximum monthly mean values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_SMAP_subset_sss_momean = ds_SMAP_subset.sss_smap.groupby(\"time.month\").mean(skipna=True)\n",
    "ds_SMAP_subset_sss_momean.min(skipna=True), ds_SMAP_subset_sss_momean.max(skipna=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### __Plot the April mean SMAP SSS__\n",
    "\n",
    "Creating a basic plot is easy using the built-in plotting capabilities of xarray:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot SMAP subset\n",
    "ds_SMAP_subset_sss_momean.sel(month=4).plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sss_min, sss_max = [ds_SMAP_subset.sss_smap.min(skipna=True).compute().data.item(), ds_SMAP_subset.sss_smap.max(skipna=True).compute().data.item()]\n",
    "sss_min, sss_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#A new figure window\n",
    "fig = plt.figure(figsize=[10,8]) \n",
    "ax = fig.add_subplot(1, 1, 1)  # specify (nrows, ncols, axnum)\n",
    "pmap = plt.axes(projection=ccrs.PlateCarree())\n",
    "\n",
    "#Necessary Variables for functions\n",
    "extent = [180-52, 180-43, -2, 6]                                           #lat/lon extents of map\n",
    "x,y = np.meshgrid(ds_SMAP_subset.lon, ds_SMAP_subset.lat)            #x, y lat/lon values for functions                                \n",
    "levels = np.linspace(sss_min, sss_max, 10)                                    #number of levels for color differentiation\n",
    "cmap='viridis'                                                       #color scheme\n",
    "t=1                                                                 #time to start with\n",
    "var = ds_SMAP_subset.sss_smap.compute()                                        #variable we will be subsetting from the GRACE-FO data\n",
    "title = str(pd.to_datetime(ds_SMAP_subset.time[t].values))           #Time of specific time step\n",
    "\n",
    "#Set up first time step\n",
    "cont = setup_map(ax, pmap, ds_SMAP_subset, x, y, var, t, cmap, levels, extent) \n",
    "\n",
    "#Make a color bar\n",
    "fig.colorbar(cont, cmap=cmap, boundaries=levels, ticks=levels, \n",
    "             orientation='horizontal', label='Sea Surface Salinity (psu)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Animate April mean SMAP SSS\n",
    "\n",
    "Like we did with GRACE, we can animate the changes for the single month of SMAP data to see how salinty changes over time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create animation for April 2019 (change the frame range for different time periods)\n",
    "ani = animation.FuncAnimation(fig, animate_ts, frames=range(0, ds_SMAP_subset.time.size-1), fargs=(\n",
    "    ax, pmap, ds_SMAP_subset, x, y, var, t, cmap, levels, extent\n",
    "),  interval=400)\n",
    "\n",
    "HTML(ani.to_html5_video())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Tutorial Summary and Additional Resources__\n",
    "\n",
    "The building blocks are now in place to do a longer time series analysis across GRACE and SMAP data, to better understand the relationship between river discharge and sea surface salinity for impact assessment. \n",
    "\n",
    "To conclude, we've searched programmatically for data archived in the PO.DAAC Earthdata Cloud over a region and time period of interest, requested the data from the Harmony API, read the data directly into `xarray` from the staged s3 location within AWS `us-west-2` without having to pull the data down into local storage, and performed subsetting and plotting in preparation for a time series analysis. \n",
    "\n",
    "There are two more sections of resources below:\n",
    "\n",
    "1. __Adding river height (pre-SWOT) data to our our analysis__\n",
    "    - Pre-SWOT data are available through the PO DAAC on-premise location, so you can continue your analysis with data both in and outside of the cloud using OPeNDAP. \n",
    "\n",
    "2. __How to set up a Jupyter Notebook running in your own EC2 instance__\n",
    "    - This same workflow can be achieved outside of the Pangeo BinderHub within an EC2 instance running your personal AWS account. More information and instructions are available below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "#### __On-prem hydro data from Pre-SWOT MEaSUREs program__\n",
    "\n",
    "Data from [**PRESWOT_HYDRO_GRRATS_L2_DAILY_VIRTUAL_STATION_HEIGHTS_V2**](https://podaac.jpl.nasa.gov/dataset/PRESWOT_HYDRO_GRRATS_L2_DAILY_VIRTUAL_STATION_HEIGHTS_V2) are not currently available on the cloud, but we can access via the PO.DAAC's on-prem OPeNDAP service (Hyrax) instead.\n",
    "\n",
    "<img src=\"https://podaac.jpl.nasa.gov/Podaac/thumbnails/PRESWOT_HYDRO_GRRATS_L2_DAILY_VIRTUAL_STATION_HEIGHTS_V2.jpg\" width=\"55%\">\n",
    "\n",
    "The guidebook explains the details of the Pre-SWOT MEaSUREs data: https://podaac-tools.jpl.nasa.gov/drive/files/allData/preswot_hydrology/L2/rivers/docs/GRRATS_user_handbookV2.pdf\n",
    "\n",
    "**Access URL for PO.DAAC on-prem OPeNDAP service**\n",
    "\n",
    "Identify an appropriate OPeNDAP endpoint through the following steps:\n",
    "\n",
    "1. Go to the project/mission page on the PO.DAAC portal (e.g. for Pre-SWOT MEaSUREs: https://podaac.jpl.nasa.gov/MEaSUREs-Pre-SWOT)\n",
    "\n",
    "2. Choose the dataset of interest. Go to the \"Data Access\" tab of the corresponding dataset landing page, which should like the OPeNDAP access link (for compatible datasets, e.g. for the daily river heights from virtual stations: https://podaac-opendap.jpl.nasa.gov/opendap/allData/preswot_hydrology/L2/rivers/daily/).\n",
    "\n",
    "3. Navigate to the desired NetCDF file and copy the endpoint (e.g. for our Amazon Basin use case we choose the South America file: https://opendap.jpl.nasa.gov/opendap/allData/preswot_hydrology/L2/rivers/daily/South_America_Amazon1kmdaily.nc).\n",
    "\n",
    "### Open netCDF file with *xarray*\n",
    "\n",
    "Open the netCDF dataset via OPeNDAP using *xarray*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_MEaSUREs = xr.open_dataset('https://opendap.jpl.nasa.gov/opendap/allData/preswot_hydrology/L2/rivers/daily/South_America_Amazon1kmdaily.nc')\n",
    "print(ds_MEaSUREs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our desired variable is height (meters above EGM2008 geoid) for this exercise, which can be subset by distance and time. Distance represents the distance from the river mouth, in this example, the Amazon estuary. Time is between April 8, 1993 and April 20, 2019.\n",
    "\n",
    "### Plot\n",
    "\n",
    "**Amazon River heights for March 16, 2018**\n",
    "\n",
    "Plot the river distances and associated heights on the map at time t=9069:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=[13,9]) \n",
    "ax = plt.axes(projection=ccrs.PlateCarree())\n",
    "ax.coastlines()\n",
    "ax.set_extent([-85, -30, -20, 20])\n",
    "ax.add_feature(cartopy.feature.RIVERS)\n",
    "\n",
    "plt.scatter(ds_MEaSUREs.lon, ds_MEaSUREs.lat, lw=1, c=ds_MEaSUREs.height[:,9069])\n",
    "plt.colorbar(label='Interpolated River Heights (m)')\n",
    "plt.clim(-10,100)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For GRACE-FO, plotting lwe_thickness[107:179,34,69] indicates time, latitude, and longitude indices corresponding to the pixel for the time period 1/2019 to 12/2019 at lat/lon (-0.7, -50). For the 2019 year, measurements of LWE thickness followd expected patterns of high volume of water from the river output into the estuary.\n",
    "\n",
    "**2011-2019 Seasonality Plots (WIP)**\n",
    "\n",
    "For GRACE-FO, plotting lwe_thickness[107:179,34,69] indicates time, latitude, and longitude indices corresponding to the pixel for the time period 8/2011 to 12/2019 at lat/lon (-0.7, -50)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot variables associated with river\n",
    "fig, ax1 = plt.subplots(figsize=[12,7])\n",
    "#plot river height\n",
    "ds_MEaSUREs.height[16,6689:9469].plot(color='darkblue')\n",
    "\n",
    "#plot LWE thickness on secondary axis\n",
    "ax2 = ax1.twinx()\n",
    "ax2.plot(subset_GRACE.time[107:179], subset_GRACE.lwe_thickness[107:179,34,69], color = 'darkorange')\n",
    "\n",
    "ax1.set_xlabel('Date')\n",
    "ax2.set_ylabel('Land Water Equivalent Thickness (cm)', color='darkorange')\n",
    "ax1.set_ylabel('River Height (m)', color='darkblue')\n",
    "ax2.legend(['GRACE-FO'], loc='upper right')\n",
    "ax1.legend(['Pre-SWOT MEaSUREs'], loc='lower right')\n",
    "\n",
    "plt.title('Amazon Estuary, 2011-2019 Lat, Lon = (-0.7, -50)')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____\n",
    "\n",
    "## __Set up for in-region access__\n",
    "\n",
    "\n",
    "__This notebook must be running within an AWS EC2 instance running in the `us-west-2` region.__\n",
    "\n",
    "For the live AGU Workshop, our BinderHub instance already takes care of steps 1 and 2, but these instructions are provided so that you can set this up in your own AWS account outside of the workshop.\n",
    "\n",
    "1. Follow tutorials 01 through 03 of the [NASA Earthdata Cloud Primer](https://earthdata.nasa.gov/learn/user-resources/webinars-and-tutorials/cloud-primer) to set up an EC2 instance within us-west-2. Ensure you are also following step 3 in the [\"Jupyter Notebooks on AWS EC2 in 12 (mostly easy) steps\"](https://medium.com/@alexjsanchez/python-3-notebooks-on-aws-ec2-in-15-mostly-easy-steps-2ec5e662c6c6) article to set the correct security group settings needed to connect your local port to your EC2’s notebook port thru SSH.\n",
    "\n",
    "2. Follow the remaining instructions in the Medium article above up until Step 11 (running Jupyter Lab). These instructions include installation of Anaconda3 (including Jupyter Lab) in your ec2 instance. Note the following updates and suggestions:\n",
    "    * Step 5: Type the following command instead of what is suggested in the article: `ssh -i \"tutorialexample.pem\" ec2-user@ec2-54-144-47-199.compute-1.amazonaws.com -L 9999:localhost:8888`. This will eliminate the need to create a ssh config file in your home directory (Step 10).\n",
    "    * As of December 2020, the most current Anaconda3 Linux distribution is: https://repo.anaconda.com/archive/Anaconda3-2020.11-Linux-x86_64.sh\n",
    "    * The Anaconda installation prompts are not the same as in the article. You will not be prompted to include Anaconda3 in your .bashrc PATH so you can skip to their step 9. Instead select \"yes\" to initialize Anaconda by running `conda init`. \n",
    "\n",
    "Before moving over to Jupyter Lab, set up your Earthdata Login authentication and Harmony access keys:\n",
    "\n",
    "3. Setup your `~/.netrc` for Earthdata Login in your ec2 instance:\n",
    "\n",
    "```\n",
    "cd ~\n",
    "touch .netrc\n",
    "echo \"machine urs.earthdata.nasa.gov login uid_goes_here password password_goes_here\" > .netrc\n",
    "chmod 0600 .netrc\n",
    "```\n",
    "\n",
    "4. Run the following in your ec2 instance terminal window to generate short-term Harmony access keys:\n",
    "\n",
    "`curl -Ln -bj https://harmony.earthdata.nasa.gov/cloud-access.sh`\n",
    "\n",
    "5. Set your environment variables based on the keys provided in step 4:\n",
    "\n",
    "`export AWS_ACCESS_KEY_ID='...\n",
    "export AWS_SECRET_ACCESS_KEY='...'\n",
    "export AWS_SESSION_TOKEN='...'\n",
    "export AWS_DEFAULT_REGION='us-west-2'`\n",
    "\n",
    "Note that these expire within 8 hours of the script generation.\n",
    "\n",
    "6. Launch jupyter lab:\n",
    "\n",
    "`jupyter lab --no-browser`\n",
    "\n",
    "Copy the URL that begins with `http://localhost:8888` into a browser window. Replace `8888` with `9999`. \n",
    "\n",
    "You should now be up and running with JupyterLab in your EC2!\n",
    "****"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
